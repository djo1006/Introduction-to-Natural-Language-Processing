{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw_les_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural machine translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "We'll use a language dataset provided by http://www.manythings.org/anki/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNvjhDyAKk3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea6e2c5-b818-4c5a-c704-62b5c43cf084"
      },
      "source": [
        "# !wget http://www.manythings.org/anki/rus-eng.zip\n",
        "!wget http://www.manythings.org/anki/swe-eng.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-07 06:53:41--  http://www.manythings.org/anki/swe-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.92.44, 172.67.186.54, 2606:4700:3030::6815:5c2c, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.92.44|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 630671 (616K) [application/zip]\n",
            "Saving to: ‘swe-eng.zip’\n",
            "\n",
            "swe-eng.zip         100%[===================>] 615.89K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-10-07 06:53:41 (6.47 MB/s) - ‘swe-eng.zip’ saved [630671/630671]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83bg17Lr-7XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f89f8b-5ef9-45c3-af74-7ff5f7beda72"
      },
      "source": [
        "# !mkdir rus-eng\n",
        "# !unzip rus-eng.zip -d rus-eng/\n",
        "!mkdir swe-eng\n",
        "!unzip swe-eng -d swe-eng/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  swe-eng.zip\n",
            "  inflating: swe-eng/_about.txt      \n",
            "  inflating: swe-eng/swe.txt         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o5L92efMMhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6ad2a3-011e-44d7-ddb7-feebc47ef676"
      },
      "source": [
        "!ls /content/swe-eng/ -lah"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.6M\n",
            "drwxr-xr-x 2 root root 4.0K Oct  7 06:53 .\n",
            "drwxr-xr-x 1 root root 4.0K Oct  7 06:53 ..\n",
            "-rw-r--r-- 1 root root 1.5K Jul 14 10:16 _about.txt\n",
            "-rw-r--r-- 1 root root 2.6M Jul 14 10:16 swe.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRVATYOgJs1b"
      },
      "source": [
        "# Download the file\n",
        "path_to_file = \"/content/swe-eng/swe.txt\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd0jw-eC3jEh"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = w.lower().strip()\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV9lZXQXNbnH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d48a0b60-8298-438e-bd6d-1ba55ea1ab5e"
      },
      "source": [
        "preprocess_sentence(\"I can't go.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<start> i can't go . <end>\""
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENG, RUS]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTbSbBz55QtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c301728-6e10-48a1-b946-554d107a2bc4"
      },
      "source": [
        "en, swe = create_dataset(path_to_file, None)\n",
        "print(en[0])\n",
        "print(swe[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> go . <end>\n",
            "<start> g . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIOn8RCNDJXG"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Limit the size of the dataset to experiment faster (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8j9g9AnIeZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c23073-395a-42de-955e-5323989f9ffb"
      },
      "source": [
        "len(en), len(swe)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19828, 19828)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnxC7q-j3jFD"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 100000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QILQkOs3jFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16606ff-2b4f-448b-eafe-fc6586bef1cd"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15862 15862 3966 3966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPmLZGMeD5q"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXukARTDd7MT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c2408f-3b1d-449b-8668-35ee8dd489af"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "1419 ----> olyckligtvis\n",
            "496 ----> pratade\n",
            "7 ----> tom\n",
            "948 ----> bredvid\n",
            "6396 ----> mun\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "1330 ----> unfortunately\n",
            "26 ----> ,\n",
            "7 ----> tom\n",
            "134 ----> let\n",
            "8 ----> the\n",
            "239 ----> cat\n",
            "57 ----> out\n",
            "21 ----> of\n",
            "8 ----> the\n",
            "553 ----> bag\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 300\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc6-NK1GtWQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b773b3b1-52e8-46a8-fbf8-e6860d0edff9"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 93]), TensorShape([64, 75]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=False,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60gSVh05Jl6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756651b9-e051-4bae-d523-4ffd0c7e7530"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5UY8wko3jFp"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden)\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKcypC0AGeLR",
        "outputId": "f51a31db-b984-440f-a455-6a3db99cc0b8"
      },
      "source": [
        "decoder_sample_x.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 5388])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y0HF-zMF_vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec834a3-2c1c-4fa1-e207-c5861e71b339"
      },
      "source": [
        "decoder_sample_h.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "source": [
        "checkpoint_dir = './training_nmt_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddefjBMa3jF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab52aa88-8316-4125-8149-ac6ea5fdb06e"
      },
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.8180\n",
            "Epoch 1 Batch 100 Loss 0.4158\n",
            "Epoch 1 Batch 200 Loss 0.3965\n",
            "Epoch 1 Loss 0.4240\n",
            "Time taken for 1 epoch 254.60595273971558 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.3297\n",
            "Epoch 2 Batch 100 Loss 0.3379\n",
            "Epoch 2 Batch 200 Loss 0.3350\n",
            "Epoch 2 Loss 0.3257\n",
            "Time taken for 1 epoch 156.57326221466064 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.2700\n",
            "Epoch 3 Batch 100 Loss 0.2625\n",
            "Epoch 3 Batch 200 Loss 0.2520\n",
            "Epoch 3 Loss 0.2610\n",
            "Time taken for 1 epoch 156.49796676635742 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2055\n",
            "Epoch 4 Batch 100 Loss 0.1975\n",
            "Epoch 4 Batch 200 Loss 0.1983\n",
            "Epoch 4 Loss 0.2085\n",
            "Time taken for 1 epoch 157.4516839981079 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1660\n",
            "Epoch 5 Batch 100 Loss 0.1470\n",
            "Epoch 5 Batch 200 Loss 0.1502\n",
            "Epoch 5 Loss 0.1624\n",
            "Time taken for 1 epoch 156.61223316192627 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1120\n",
            "Epoch 6 Batch 100 Loss 0.1257\n",
            "Epoch 6 Batch 200 Loss 0.1343\n",
            "Epoch 6 Loss 0.1235\n",
            "Time taken for 1 epoch 156.12342977523804 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0833\n",
            "Epoch 7 Batch 100 Loss 0.0909\n",
            "Epoch 7 Batch 200 Loss 0.0888\n",
            "Epoch 7 Loss 0.0921\n",
            "Time taken for 1 epoch 156.84375667572021 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0641\n",
            "Epoch 8 Batch 100 Loss 0.0739\n",
            "Epoch 8 Batch 200 Loss 0.0770\n",
            "Epoch 8 Loss 0.0681\n",
            "Time taken for 1 epoch 157.38471698760986 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0498\n",
            "Epoch 9 Batch 100 Loss 0.0500\n",
            "Epoch 9 Batch 200 Loss 0.0526\n",
            "Epoch 9 Loss 0.0495\n",
            "Time taken for 1 epoch 156.77385473251343 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0320\n",
            "Epoch 10 Batch 100 Loss 0.0296\n",
            "Epoch 10 Batch 200 Loss 0.0420\n",
            "Epoch 10 Loss 0.0362\n",
            "Time taken for 1 epoch 157.24084973335266 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0229\n",
            "Epoch 11 Batch 100 Loss 0.0219\n",
            "Epoch 11 Batch 200 Loss 0.0288\n",
            "Epoch 11 Loss 0.0268\n",
            "Time taken for 1 epoch 157.06419157981873 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0182\n",
            "Epoch 12 Batch 100 Loss 0.0245\n",
            "Epoch 12 Batch 200 Loss 0.0236\n",
            "Epoch 12 Loss 0.0201\n",
            "Time taken for 1 epoch 157.38475155830383 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0119\n",
            "Epoch 13 Batch 100 Loss 0.0114\n",
            "Epoch 13 Batch 200 Loss 0.0165\n",
            "Epoch 13 Loss 0.0155\n",
            "Time taken for 1 epoch 157.17322158813477 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0111\n",
            "Epoch 14 Batch 100 Loss 0.0142\n",
            "Epoch 14 Batch 200 Loss 0.0155\n",
            "Epoch 14 Loss 0.0126\n",
            "Time taken for 1 epoch 157.70354342460632 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0077\n",
            "Epoch 15 Batch 100 Loss 0.0089\n",
            "Epoch 15 Batch 200 Loss 0.0090\n",
            "Epoch 15 Loss 0.0102\n",
            "Time taken for 1 epoch 156.8156545162201 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0075\n",
            "Epoch 16 Batch 100 Loss 0.0069\n",
            "Epoch 16 Batch 200 Loss 0.0093\n",
            "Epoch 16 Loss 0.0087\n",
            "Time taken for 1 epoch 157.06644344329834 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0075\n",
            "Epoch 17 Batch 100 Loss 0.0055\n",
            "Epoch 17 Batch 200 Loss 0.0088\n",
            "Epoch 17 Loss 0.0078\n",
            "Time taken for 1 epoch 156.72486424446106 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0047\n",
            "Epoch 18 Batch 100 Loss 0.0027\n",
            "Epoch 18 Batch 200 Loss 0.0058\n",
            "Epoch 18 Loss 0.0071\n",
            "Time taken for 1 epoch 156.86159491539001 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0073\n",
            "Epoch 19 Batch 100 Loss 0.0055\n",
            "Epoch 19 Batch 200 Loss 0.0123\n",
            "Epoch 19 Loss 0.0066\n",
            "Time taken for 1 epoch 156.25607824325562 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0063\n",
            "Epoch 20 Batch 100 Loss 0.0058\n",
            "Epoch 20 Batch 200 Loss 0.0110\n",
            "Epoch 20 Loss 0.0065\n",
            "Time taken for 1 epoch 156.56112217903137 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0035\n",
            "Epoch 21 Batch 100 Loss 0.0068\n",
            "Epoch 21 Batch 200 Loss 0.0045\n",
            "Epoch 21 Loss 0.0070\n",
            "Time taken for 1 epoch 156.01200604438782 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0077\n",
            "Epoch 22 Batch 100 Loss 0.0064\n",
            "Epoch 22 Batch 200 Loss 0.0101\n",
            "Epoch 22 Loss 0.0079\n",
            "Time taken for 1 epoch 156.27362060546875 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0066\n",
            "Epoch 23 Batch 100 Loss 0.0073\n",
            "Epoch 23 Batch 200 Loss 0.0126\n",
            "Epoch 23 Loss 0.0101\n",
            "Time taken for 1 epoch 156.12972927093506 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0066\n",
            "Epoch 24 Batch 100 Loss 0.0112\n",
            "Epoch 24 Batch 200 Loss 0.0109\n",
            "Epoch 24 Loss 0.0116\n",
            "Time taken for 1 epoch 156.36968207359314 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0067\n",
            "Epoch 25 Batch 100 Loss 0.0090\n",
            "Epoch 25 Batch 200 Loss 0.0102\n",
            "Epoch 25 Loss 0.0102\n",
            "Time taken for 1 epoch 156.1119408607483 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0071\n",
            "Epoch 26 Batch 100 Loss 0.0122\n",
            "Epoch 26 Batch 200 Loss 0.0092\n",
            "Epoch 26 Loss 0.0081\n",
            "Time taken for 1 epoch 156.38505363464355 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0134\n",
            "Epoch 27 Batch 100 Loss 0.0052\n",
            "Epoch 27 Batch 200 Loss 0.0056\n",
            "Epoch 27 Loss 0.0062\n",
            "Time taken for 1 epoch 155.6202254295349 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0045\n",
            "Epoch 28 Batch 100 Loss 0.0041\n",
            "Epoch 28 Batch 200 Loss 0.0053\n",
            "Epoch 28 Loss 0.0052\n",
            "Time taken for 1 epoch 155.8932490348816 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0064\n",
            "Epoch 29 Batch 100 Loss 0.0058\n",
            "Epoch 29 Batch 200 Loss 0.0064\n",
            "Epoch 29 Loss 0.0046\n",
            "Time taken for 1 epoch 155.89910769462585 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0023\n",
            "Epoch 30 Batch 100 Loss 0.0044\n",
            "Epoch 30 Batch 200 Loss 0.0016\n",
            "Epoch 30 Loss 0.0045\n",
            "Time taken for 1 epoch 156.0283966064453 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0030\n",
            "Epoch 31 Batch 100 Loss 0.0010\n",
            "Epoch 31 Batch 200 Loss 0.0051\n",
            "Epoch 31 Loss 0.0045\n",
            "Time taken for 1 epoch 155.66456866264343 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0015\n",
            "Epoch 32 Batch 100 Loss 0.0049\n",
            "Epoch 32 Batch 200 Loss 0.0054\n",
            "Epoch 32 Loss 0.0048\n",
            "Time taken for 1 epoch 156.29189014434814 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0067\n",
            "Epoch 33 Batch 100 Loss 0.0025\n",
            "Epoch 33 Batch 200 Loss 0.0033\n",
            "Epoch 33 Loss 0.0054\n",
            "Time taken for 1 epoch 156.57961869239807 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0065\n",
            "Epoch 34 Batch 100 Loss 0.0042\n",
            "Epoch 34 Batch 200 Loss 0.0066\n",
            "Epoch 34 Loss 0.0064\n",
            "Time taken for 1 epoch 156.91323471069336 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0065\n",
            "Epoch 35 Batch 100 Loss 0.0151\n",
            "Epoch 35 Batch 200 Loss 0.0089\n",
            "Epoch 35 Loss 0.0100\n",
            "Time taken for 1 epoch 156.77158546447754 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0124\n",
            "Epoch 36 Batch 100 Loss 0.0088\n",
            "Epoch 36 Batch 200 Loss 0.0173\n",
            "Epoch 36 Loss 0.0133\n",
            "Time taken for 1 epoch 156.88417148590088 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0102\n",
            "Epoch 37 Batch 100 Loss 0.0130\n",
            "Epoch 37 Batch 200 Loss 0.0146\n",
            "Epoch 37 Loss 0.0109\n",
            "Time taken for 1 epoch 156.4387457370758 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0054\n",
            "Epoch 38 Batch 100 Loss 0.0087\n",
            "Epoch 38 Batch 200 Loss 0.0091\n",
            "Epoch 38 Loss 0.0072\n",
            "Time taken for 1 epoch 157.03446912765503 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0021\n",
            "Epoch 39 Batch 100 Loss 0.0064\n",
            "Epoch 39 Batch 200 Loss 0.0080\n",
            "Epoch 39 Loss 0.0053\n",
            "Time taken for 1 epoch 156.13829398155212 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0093\n",
            "Epoch 40 Batch 100 Loss 0.0056\n",
            "Epoch 40 Batch 200 Loss 0.0069\n",
            "Epoch 40 Loss 0.0044\n",
            "Time taken for 1 epoch 155.9317696094513 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0015\n",
            "Epoch 41 Batch 100 Loss 0.0052\n",
            "Epoch 41 Batch 200 Loss 0.0044\n",
            "Epoch 41 Loss 0.0041\n",
            "Time taken for 1 epoch 155.36253261566162 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0023\n",
            "Epoch 42 Batch 100 Loss 0.0052\n",
            "Epoch 42 Batch 200 Loss 0.0046\n",
            "Epoch 42 Loss 0.0039\n",
            "Time taken for 1 epoch 155.8495593070984 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0026\n",
            "Epoch 43 Batch 100 Loss 0.0007\n",
            "Epoch 43 Batch 200 Loss 0.0045\n",
            "Epoch 43 Loss 0.0039\n",
            "Time taken for 1 epoch 155.33408212661743 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0021\n",
            "Epoch 44 Batch 100 Loss 0.0014\n",
            "Epoch 44 Batch 200 Loss 0.0032\n",
            "Epoch 44 Loss 0.0037\n",
            "Time taken for 1 epoch 155.7319049835205 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0086\n",
            "Epoch 45 Batch 100 Loss 0.0015\n",
            "Epoch 45 Batch 200 Loss 0.0136\n",
            "Epoch 45 Loss 0.0039\n",
            "Time taken for 1 epoch 155.41631722450256 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0020\n",
            "Epoch 46 Batch 100 Loss 0.0026\n",
            "Epoch 46 Batch 200 Loss 0.0049\n",
            "Epoch 46 Loss 0.0039\n",
            "Time taken for 1 epoch 155.93154335021973 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0043\n",
            "Epoch 47 Batch 100 Loss 0.0027\n",
            "Epoch 47 Batch 200 Loss 0.0086\n",
            "Epoch 47 Loss 0.0041\n",
            "Time taken for 1 epoch 155.5737202167511 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0005\n",
            "Epoch 48 Batch 100 Loss 0.0056\n",
            "Epoch 48 Batch 200 Loss 0.0092\n",
            "Epoch 48 Loss 0.0049\n",
            "Time taken for 1 epoch 156.01232147216797 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0009\n",
            "Epoch 49 Batch 100 Loss 0.0068\n",
            "Epoch 49 Batch 200 Loss 0.0114\n",
            "Epoch 49 Loss 0.0083\n",
            "Time taken for 1 epoch 156.4258315563202 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0162\n",
            "Epoch 50 Batch 100 Loss 0.0239\n",
            "Epoch 50 Batch 200 Loss 0.0209\n",
            "Epoch 50 Loss 0.0191\n",
            "Time taken for 1 epoch 156.9398205280304 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl9zUHzg3jGI"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJpT9D5_OgP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd259ea3-927e-445b-a019-1b714a75a1b4"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa3ae5c3210>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrAM0FDomq3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ec038d-5bb8-4757-c34c-e7fba6b8301a"
      },
      "source": [
        "translate('Det är trevligt här.')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> det r trevligt h r . <end>\n",
            "Predicted translation: it's time for this . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bhFfwcIMX5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edda7a02-6da6-459e-b9a6-55a9acc3aacc"
      },
      "source": [
        "translate('Jag kommer inte att kunna gå.')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> jag kommer inte att kunna g . <end>\n",
            "Predicted translation: i won't stay . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSx2iM36EZQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124a6081-dc2b-49dc-a040-7b6e8766fcd3"
      },
      "source": [
        "translate(u'Är du fortfarande hemma?')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> r du fortfarande hemma ? <end>\n",
            "Predicted translation: are you still at home ? <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3LLCx3ZE0Ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3dfeede-ccfd-45f1-9a8f-4c7d5d5cc4ef"
      },
      "source": [
        "translate(u'Är du fortfarande hemma?')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> r du fortfarande hemma ? <end>\n",
            "Predicted translation: are you still at home ? <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUQVLVqUE1YW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2188544f-43e8-43df-bd5b-22d40c55358b"
      },
      "source": [
        "translate(u'Prova detta.')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> prova detta . <end>\n",
            "Predicted translation: don't eat . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f09_hUFx9EJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba95f7b-664c-4b15-be70-27f1d7ef35c8"
      },
      "source": [
        "translate(u'Jag älskar det när det snöar.')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> jag lskar det n r det sn ar . <end>\n",
            "Predicted translation: i love that tattoo . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7c5p8rmkHQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8bcd2ab-a214-41ed-b49f-00f5005ea94a"
      },
      "source": [
        "translate(u'Jag gör aldrig det.')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> jag g r aldrig det . <end>\n",
            "Predicted translation: i'll never get it . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdXES85KkTVS"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    }
  ]
}